[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Experience with R\n\nhist(Titanic)\n\n\n\n\nWord cloud of Winston Churchill’s “This was their finest hour”, June 18, 1940\n\n\n\n\nwordcloud(names(wordCounts),wordCounts, min.freq=6,random.order=FALSE, max.words=50,scale=c(3,.5), rot.per=0.35,colors=brewer.pal(8,\"Dark2\"))"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "G Argueta",
    "section": "",
    "text": "This is a Quarto website. This website was developed to host my projects and data stories.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\nIntroduction\nThe site was built and developed in R Studio for the purpose of retaining projects, papers, and data models.\n\n\nR Scripts\n\nplot(iris, pch=20, cex=.5, col=\"firebrick\")\n\n\n\n\n\nplot(cars, xlab=\"Speed\", ylab=\"Distance\", col=\"blue\")\n\n\n\n\n\nhist(Titanic)"
  },
  {
    "objectID": "data_topics.html",
    "href": "data_topics.html",
    "title": "Data Topics",
    "section": "",
    "text": "Time Series Data and Break Point Analysis\nAreas of research concerned with change should consider time, and effects of the past, as potentially important variables. Cross-sectional studies take measurements from a single point in time, while longitudinal studies take measurements in multiple waves over time. Reference to longitudinal data is common, but here the term longitudinal only is attributable to studies or research designs. Within a longitudinal design, there are repeated measurement and time series data. Repeat measurement is when measurements are taken over many waves but may not be collected at standard time intervals. Time series data are collected on regular intervals and should contain at least thirty measurements. Panel data may be repeated measurement or time series. The key feature of panel data is the measurement of the same entities over time, rather than collecting measurements from the same population even if the entities measured differ.\nThere are no universal rules or definitions for distinguishing between the terms longitudinal, time series, and panel with regard to study or data types, but these are the definitions used to guide the discussion on this site.\nPlanned interventions with study groups in comparison to control groups that do not receive an intervention are a common method used for testing hypotheses in longitudinal studies. Break point analysis is a method used to identify unintended or natural interventions that cause change in situations that were not designed or controlled. Break point analysis can also assist in identifying or verifying true changes when compared with stated beliefs about the drivers of change. Knowing the timing of change ensures that perceived drivers are temporally plausible."
  },
  {
    "objectID": "studies.html",
    "href": "studies.html",
    "title": "Study Results",
    "section": "",
    "text": "Role of Normative Ethics in Decision Making\nStudy of Support for the 2022 Student Loan Forgiveness Program\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExploratory Data Analysis\nReview of US Bureau of Labor Statistics Occupation Projections - 2020 through 2030\nNote - Bureau of Labor Statistics data elements for education, training, and experience are ordered so that higher levels of a given variable are closer to zero. For example, lower levels of education are identified with higher numbers (4 through 8 are less than a bachelor’s degree) and higher levels of education are lower numbers (1 through 3 are bachelor’s degree requirements or higher). The result of this coding methodology is output that is not intuitive to interpret.\nJobs requiring higher levels of training, experience, and education are not expected to experience as much volatility (large increases or decreases) as jobs requiring less training, experience, and education."
  },
  {
    "objectID": "words_action.html",
    "href": "words_action.html",
    "title": "Words in Action",
    "section": "",
    "text": "Word Clouds\nWinston Churchill’s “This was their finest hour”, June 18, 1940\n\nwordcloud(names(wordCounts),wordCounts, min.freq=6,random.order=FALSE, max.words=50,scale=c(3,.5), rot.per=0.35,colors=brewer.pal(8,\"Dark2\"))\n\n\n\n\n\n\nQuotes for Thought\n“One can govern people only by showing them the future; a head of state is a merchant of hope.” attributed to Napoleon Bonaparte\n“He who receives an idea from me, receives instruction himself without lessening mine; as he who lights his taper at mine, receives light without darkening me.” attributed to Thomas Jefferson\n“Our lives begin to end the day we become silent about things that matter.” attributed to Martin Luther King Jr.\n“Religion isn’t the mystery of incarnation; it is the mystery of the social order” attributed to Napoleon Bonaparte\n“The chains of habit are too light to be felt until they are too heavy to be broken.” attributed to Warren Buffett\n“If you think education is expensive, try ignorance.” attributed to Robert Orben\n“Knowledge itself is power” attributed to Sir Francis Bacon ~ Ipsa scientia potestas est"
  },
  {
    "objectID": "assignments.html",
    "href": "assignments.html",
    "title": "Knowledge Mining",
    "section": "",
    "text": "Comparison of Perspectives - What is the Objective of Research? Understanding versus Prediction\nThe following is a review of articles authors be Breiman (2001) and Shmueli (2010) regarding the usefulness and applicability of explanatory (or data) statistical models as opposed to predictive (or algorithmic) models.\nBreiman’s (2001) assessment of the two cultures in statistical modeling begins by illustrating that factors pass through real world functions to produce outcomes. The real world functions are characterized as nature’s black box which links predictor and response variables. This article’s focus is the comparison of data (or explanatory) models such as regressions, and random or random-like trials; with algorithmic (or predictive) models such as support vectors, forests, and deep learners. Breiman (2001) seems more critical of data models than their algorithmic counterparts and describes three types of issues encountered in the development of data models. The issues include:\n\nThe Rashomon or multiplicity problem – there may be several data model equations (that is, different predictors are included in various models), that display similar predictive abilities. If different variables can be included to result in the same predictive strength, how can a researcher ensure the selected variables are the most relevant to the question at hand? A data model researcher may suggest that this is the function and purpose of incorporating theory into modeling, but that is not discussed in the article.\nThe Occam or simplicity versus accuracy problem – in many cases, a decision will need to be made during the model development process between simplicity (associated with interpretability) and accuracy. For data modeling, the ability to understand and interpret results are likely to take priority. In algorithmic models where prediction is the focus, the desire for accuracy may be paramount, regardless of the complexity of the model.\nThe Bellman or dimensionality problem – Also known by the ominous expression, “curse of dimensionality”, this problem is encountered when the number of predictors necessary to reach a reasonable level of predictive power is high. This problem not only contributes to a model’s complexity and inhibits interpretability, it also complicates controls during research design and implementation.\n\nThe article outlines specific examples of statistical modeling that could be summarized with the adage “garbage in, garbage out”. In addition, the article develops what seems to be a value judgement where algorithmic modeling techniques are more useful than data modeling techniques. This may be because of the authors background in consulting, where applied research efforts may be more prevalent than the basic research seen in publications and university settings.\nAlgorithmic models may be able to more accurately predict outcomes based on what has past, but without an understanding of the underlying mechanisms driving change, it may not be possible to predict significant deviation from that past or understand how to influence the future. The ability to predict has many applications within warning systems, marketing, and diagnosis; but the ability to contravene or influence outcomes requires an understanding of the primary mechanisms at play within a given process. Understanding is not created by prediction alone, which is a likely reason that purely predictive tools are rarely seen in publications. Algorithmic models may struggle to answer the question, “So what?”.\nShmueli (2010) notes that research may have more than one objective. It may be prediction and it may be identifying intervention points for understanding influences within processes. The article seeks to clarify the difference and different applications of explanatory (data) and predictive (algorithmic) models. Shmueli (2010) also notes that confusion surround these modeling methods has led to gaps between academia and practice, with each set of organizations pursuing different types of research. Academia’s focus has been basic sciences while practice has utilized applied sciences.\nThe article provides several reasons for including predictive modeling in a research toolbox.\n\nThe ubiquitous nature of data in the information age may present new possibilities with the identification of new or better predictors for testing existing theories.\nPredictive models as exploratory analysis could lead to new theories for explaining natural processes.\nPredictive models could be used to assess the distance between current explanatory models and reality.\n\nShmueli (2010) also covers the differences between basic and applied sciences in more detail. These terms have also been referred to as basic versus applied research, or research versus development. Basic research is undertaken to expand understanding in a certain field through systematic study. Applied research is the use of that understanding to create useful products or services. These research categories are not substitutes, they are compliments which rely on each other. If understanding was not expected to eventually benefit society, organizations and governments would have little incentive to fund basic research. If basic research was not producing new understanding, research for the application of understanding would not be possible. Applied research would struggle to use understanding if new knowledge was not being produced regularly.\nAlthough the author contrasts the design and implementation processes for explanatory and predictive models, Shmueli (2010) captures the key difference in these concepts in the description of how the three components of research are utilized. The three components being predictors, outcomes, and the function of the predictors. Predictors and outcomes are described as tools for the development of an estimated function for explanation. Whereas in predictive models, the function and predictors are tools for producing estimated outcomes.\nShmueli’s (2010) arguments about the merits, uses, and purposeful application of both predictive and explanatory models are more compelling and more useful for data science practitioners across the academia-practice spectrum. Prediction and understanding are both useful endeavors and require different tools. In this way, big data (algorithmic or predictive) and small data (data or explanatory) cultures may benefit from the clarification of when the use of each method is justifiable based on the objectives and purpose of the research being conducted. This may be more useful than attempting to assign a culture or model value without regard for the needs of a particular study or project. When the job requires a nail, a hammer is the justifiable tool; for a screw, use a screwdriver.\nBreiman, L. (2001). Statistical modeling: The two cultures. Statistical Science, 16(3), 199–231. https://doi.org/10.1214/ss/1009213726\nShmueli, G. (2010). To explain or to predict? Statistical Science, 25(3), 289–310. https://doi.org/10.1214/10-sts330"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "CV and Contact Information",
    "section": "",
    "text": "Connect with Me on Linkedin\nhttps://www.linkedin.com/in/greg-argueta-cpa-mba-6b147b15"
  }
]