---
title: "Knowledge Mining"
---

### **Comparison of Perspectives - What is the Objective of Research? Understanding versus Prediction**

The following is a review of articles authors be Breiman (2001) and Shmueli (2010) regarding the usefulness and applicability of explanatory (or data) statistical models as opposed to predictive (or algorithmic) models. 

Breiman’s (2001) assessment of the two cultures in statistical modeling begins by illustrating that factors pass through real world functions to produce outcomes. The real world functions are characterized as nature’s black box which links predictor and response variables.  This article’s focus is the comparison of data (or explanatory) models such as regressions, and random or random-like trials; with algorithmic (or predictive) models such as support vectors, forests, and deep learners. Breiman (2001) seems more critical of data models than their algorithmic counterparts and describes three types of issues encountered in the development of data models. The issues include: 

  1.	The Rashomon or multiplicity problem – there may be several data model equations (that is, different predictors are included in various models), that display similar predictive abilities. If different variables can be included to result in the same predictive strength, how can a researcher ensure the selected variables are the most relevant to the question at hand? A data model researcher may suggest that this is the function and purpose of incorporating theory into modeling, but that is not discussed in the article.
  2.	The Occam or simplicity versus accuracy problem – in many cases, a decision will need to be made during the model development process between simplicity (associated with interpretability) and accuracy. For data modeling, the ability to understand and interpret results are likely to take priority. In algorithmic models where prediction is the focus, the desire for accuracy may be paramount, regardless of the complexity of the model.
  3.	The Bellman or dimensionality problem – Also known by the ominous expression, “curse of dimensionality”, this problem is encountered when the number of predictors necessary to reach a reasonable level of predictive power is high. This problem not only contributes to a model’s complexity and inhibits interpretability, it also complicates controls during research design and implementation. 
  
The article outlines specific examples of statistical modeling that could be summarized with the adage “garbage in, garbage out”. In addition, the article develops what seems to be a value judgement where algorithmic modeling techniques are more useful than data modeling techniques. This may be because of the authors background in consulting, where applied research efforts may be more prevalent than the basic research seen in publications and university settings.  

Algorithmic models may be able to more accurately predict outcomes based on what has past, but without an understanding of the underlying mechanisms driving change, it may not be possible to predict significant deviation from that past or understand how to influence the future. The ability to predict has many applications within warning systems, marketing, and diagnosis; but the ability to contravene or influence outcomes requires an understanding of the primary mechanisms at play within a given process. Understanding is not created by prediction alone, which is a likely reason that purely predictive tools are rarely seen in publications. Algorithmic models may struggle to answer the question, “So what?”. 

Shmueli (2010) notes that research may have more than one objective. It may be prediction and it may be identifying intervention points for understanding influences within processes. The article seeks to clarify the difference and different applications of explanatory (data) and predictive (algorithmic) models. Shmueli (2010) also notes that confusion surround these modeling methods has led to gaps between academia and practice, with each set of organizations pursuing different types of research. Academia’s focus has been basic sciences while practice has utilized applied sciences.

The article provides several reasons for including predictive modeling in a research toolbox. 

  1.	The ubiquitous nature of data in the information age may present new possibilities with the identification of new or better predictors for testing existing theories.
  2.	Predictive models as exploratory analysis could lead to new theories for explaining natural processes.
  3.	Predictive models could be used to assess the distance between current explanatory models and reality.
  
Shmueli (2010) also covers the differences between basic and applied sciences in more detail. These terms have also been referred to as basic versus applied research, or research versus development. Basic research is undertaken to expand understanding in a certain field through systematic study. Applied research is the use of that understanding to create useful products or services. These research categories are not substitutes, they are compliments which rely on each other. If understanding was not expected to eventually benefit society, organizations and governments would have little incentive to fund basic research. If basic research was not producing new understanding, research for the application of understanding would not be possible. Applied research would struggle to use understanding if new knowledge was not being produced regularly.

Although the author contrasts the design and implementation processes for explanatory and predictive models, Shmueli (2010) captures the key difference in these concepts in the description of how the three components of research are utilized. The three components being predictors, outcomes, and the function of the predictors. Predictors and outcomes are described as tools for the development of an estimated function for explanation. Whereas in predictive models, the function and predictors are tools for producing estimated outcomes.   

Shmueli’s (2010) arguments about the merits, uses, and purposeful application of both predictive and explanatory models are more compelling and more useful for data science practitioners across the academia-practice spectrum. Prediction and understanding are both useful endeavors and require different tools. In this way, big data (algorithmic or predictive) and small data (data or explanatory) cultures may benefit from the clarification of when the use of each method is justifiable based on the objectives and purpose of the research being conducted. This may be more useful than attempting to assign a culture or model value without regard for the needs of a particular study or project. When the job requires a nail, a hammer is the justifiable tool; for a screw, use a screwdriver.

Breiman, L. (2001). Statistical modeling: The two cultures. Statistical Science, 16(3), 199–231. https://doi.org/10.1214/ss/1009213726 

Shmueli, G. (2010). To explain or to predict? Statistical Science, 25(3), 289–310. https://doi.org/10.1214/10-sts330 

### 
